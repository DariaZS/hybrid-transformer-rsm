{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4dde2e",
   "metadata": {},
   "source": [
    "# Rolling State Memory (RSM) - Experimental Notebook\n",
    "\n",
    "**Implementation using modular architecture**\n",
    "\n",
    "This notebook demonstrates RSM training and evaluation using the clean architecture from `hybrid_transformer1.py`.\n",
    "\n",
    "**Key improvements:**\n",
    "- Modular imports from `hybrid_transformer1.py`\n",
    "- Multiple dataset/tokenization options\n",
    "- Clean experimental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070ae99",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "Import all necessary components from our architecture module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3356dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì hybrid_transformer1 module loaded.\n",
      "Device: cpu\n",
      "PyTorch version: 2.0.0\n",
      "‚úì Imported RSM architecture from hybrid_transformer1.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# Import ALL RSM components from our clean architecture\n",
    "from hybrid_transformer1 import (\n",
    "    # NNDL Core Functions (Section 1)\n",
    "    scaled_dot_attention,\n",
    "    PositionalEncoding,\n",
    "    MLP,\n",
    "    CausalSelfAttention,\n",
    "    \n",
    "    # Memory Components (Section 2)\n",
    "    CrossAttention,\n",
    "    GatedSSM,\n",
    "    GlobalSyncLayer,\n",
    "    \n",
    "    # Architecture (Section 3)\n",
    "    HybridTransformerBlock,\n",
    "    HybridTransformer,\n",
    "    \n",
    "    # Training Utilities (Section 4)\n",
    "    train_rsm_epoch,\n",
    "    \n",
    "    # Generation Utilities (Section 5)\n",
    "    generate_with_rsm,\n",
    "    \n",
    "    # Dataset Utilities (Section 6)\n",
    "    ChunkedSequenceDataset,\n",
    "    \n",
    "    # Helper Functions (Section 7)\n",
    "    create_rsm_model,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    count_parameters,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"‚úì Imported RSM architecture from hybrid_transformer1.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d389f7",
   "metadata": {},
   "source": [
    "## 2. Dataset & Tokenization Options\n",
    "\n",
    "Choose your dataset and tokenization scheme based on your experiment goals.\n",
    "\n",
    "### üìö Dataset Options:\n",
    "\n",
    "#### **Option 1: TinyStories** (Microsoft Research, 2023) ‚≠ê \n",
    "- **Paper:** \"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?\"\n",
    "- **Authors:** Eldan & Li (2023)\n",
    "- **Size:** 2.1M synthetic stories generated by GPT-4\n",
    "- **Length:** 500-2000 tokens per story\n",
    "- **Use case:** Standard benchmark for small language models\n",
    "- **Citation:** https://arxiv.org/abs/2305.07759\n",
    "\n",
    "#### **Option 2: Tiny Shakespeare** (Karpathy's char-rnn) ‚≠ê\n",
    "- **Source:** Complete works of Shakespeare\n",
    "- **Size:** ~1MB text, ~1M characters\n",
    "- **Vocab:** ~65 unique characters\n",
    "- **Use case:** Fast experiments, character-level modeling benchmark\n",
    "- **Citation:** https://github.com/karpathy/char-rnn\n",
    "\n",
    "#### **Option 3: WikiText-103** (Salesforce Research)\n",
    "- **Paper:** \"Pointer Sentinel Mixture Models\"\n",
    "- **Authors:** Merity et al. (2016)\n",
    "- **Size:** 103M tokens from Wikipedia\n",
    "- **Use case:** Long-form text, established benchmark\n",
    "- **Citation:** https://arxiv.org/abs/1609.07843\n",
    "\n",
    "#### **Option 4: Custom Text**\n",
    "- Load your own `.txt` files\n",
    "- Suitable for domain-specific applications\n",
    "\n",
    "---\n",
    "\n",
    "### üî§ Tokenization Options:\n",
    "\n",
    "#### **Method 1: BPE (Byte-Pair Encoding)** ‚≠ê \n",
    "- **Tokenizer:** GPT-2 (50K vocab)\n",
    "- **Efficiency:** ~4 characters per token\n",
    "- **Pros:** Semantic units, no OOV, pretrained\n",
    "- **Cons:** Large vocabulary\n",
    "\n",
    "#### **Method 2: SentencePiece** (Custom vocab size)\n",
    "- **Vocab size:** Configurable (2K-8K typical)\n",
    "- **Pros:** Trainable on your data, balanced\n",
    "- **Cons:** Requires training step\n",
    "\n",
    "#### **Method 3: Character-level**\n",
    "- **Vocab size:** ~100 characters\n",
    "- **Pros:** Simple, no OOV, works for any text\n",
    "- **Cons:** Very long sequences, harder to learn patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e885b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET LOADING - Choose one option\n",
    "# ============================================================================\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# OPTION 1: TinyStories + GPT-2 BPE Tokenizer (COMMENTED OUT - using Shakespeare)\n",
    "# ----------------------------------------------------------------------------\n",
    "# Paper: \"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?\"\n",
    "# Eldan & Li, Microsoft Research, 2023\n",
    "# https://arxiv.org/abs/2305.07759\n",
    "\n",
    "# try:\n",
    "#     from datasets import load_dataset\n",
    "#     from transformers import GPT2Tokenizer\n",
    "#     \n",
    "#     print(\"=\" * 80)\n",
    "#     print(\"LOADING TINYSTORIES DATASET\")\n",
    "#     print(\"=\" * 80)\n",
    "#     \n",
    "#     # Load dataset from HuggingFace\n",
    "#     tinystories = load_dataset(\"roneneldan/TinyStories\", split=\"train[:500]\")\n",
    "#     print(f\"‚úì Loaded {len(tinystories)} stories from TinyStories\")\n",
    "#     \n",
    "#     # Load GPT-2 tokenizer (50,257 vocab)\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "#     vocab_size = len(tokenizer)\n",
    "#     \n",
    "#     print(f\"‚úì GPT-2 BPE Tokenizer loaded\")\n",
    "#     print(f\"  Vocabulary size: {vocab_size:,} tokens\")\n",
    "#     print(f\"  Tokenization: Byte-Pair Encoding\")\n",
    "#     \n",
    "#     # Tokenize all stories\n",
    "#     all_tokens = []\n",
    "#     for story in tinystories:\n",
    "#         tokens = tokenizer.encode(story['text'])\n",
    "#         all_tokens.extend(tokens)\n",
    "#     \n",
    "#     print(f\"‚úì Tokenized {len(all_tokens):,} tokens total\")\n",
    "#     print(\"=\" * 80)\n",
    "#     \n",
    "#     DATASET_NAME = \"TinyStories\"\n",
    "#     TOKENIZATION = \"BPE (GPT-2)\"\n",
    "#     \n",
    "# except ImportError:\n",
    "#     print(\"‚ö† HuggingFace libraries not installed\")\n",
    "#     print(\"Install with: pip install datasets transformers\")\n",
    "#     print(\"\\nFalling back to character-level tokenization...\")\n",
    "#     \n",
    "#     # Fallback to character-level with sample text\n",
    "#     sample_text = \"Once upon a time, there was a little girl. \" * 100\n",
    "#     \n",
    "#     # Character-level tokenizer\n",
    "#     chars = sorted(list(set(sample_text)))\n",
    "#     char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "#     idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "#     \n",
    "#     all_tokens = [char_to_idx[ch] for ch in sample_text]\n",
    "#     vocab_size = len(chars)\n",
    "#     \n",
    "#     print(f\"‚úì Character-level tokenization\")\n",
    "#     print(f\"  Vocabulary size: {vocab_size} characters\")\n",
    "#     print(f\"  Total tokens: {len(all_tokens):,}\")\n",
    "#     \n",
    "#     DATASET_NAME = \"Sample Text\"\n",
    "#     TOKENIZATION = \"Character-level\"\n",
    "# \n",
    "# print(f\"\\nüìä Dataset: {DATASET_NAME}\")\n",
    "# print(f\"üî§ Tokenization: {TOKENIZATION}\")\n",
    "# print(f\"üìù Vocabulary: {vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea78799",
   "metadata": {},
   "source": [
    "## 2b. Alternative: Tiny Shakespeare Dataset\n",
    "\n",
    "Uncomment this cell to use Tiny Shakespeare instead of TinyStories (faster, simpler, character-level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf4cddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING TINY SHAKESPEARE\n",
      "================================================================================\n",
      "‚úì Downloaded 1,115,394 characters\n",
      "‚úì Character-level tokenization\n",
      "  Vocabulary: 65 unique characters\n",
      "  Total tokens: 1,115,394\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset: Tiny Shakespeare\n",
      "üî§ Tokenization: Character-level\n",
      "üìù Vocabulary: 65 tokens\n",
      "\n",
      "üìñ Preview:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "‚úì Downloaded 1,115,394 characters\n",
      "‚úì Character-level tokenization\n",
      "  Vocabulary: 65 unique characters\n",
      "  Total tokens: 1,115,394\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset: Tiny Shakespeare\n",
      "üî§ Tokenization: Character-level\n",
      "üìù Vocabulary: 65 tokens\n",
      "\n",
      "üìñ Preview:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TINY SHAKESPEARE DATASET - ACTIVE\n",
    "# ============================================================================\n",
    "# Classic benchmark - Complete works of Shakespeare (~1MB, ~1M characters)\n",
    "# Perfect for quick experiments and testing\n",
    "# https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "import requests\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING TINY SHAKESPEARE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Download Tiny Shakespeare\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "print(f\"‚úì Downloaded {len(text):,} characters\")\n",
    "\n",
    "# Character-level tokenization\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "all_tokens = [char_to_idx[ch] for ch in text]\n",
    "vocab_size = len(chars)\n",
    "\n",
    "DATASET_NAME = \"Tiny Shakespeare\"\n",
    "TOKENIZATION = \"Character-level\"\n",
    "\n",
    "print(f\"‚úì Character-level tokenization\")\n",
    "print(f\"  Vocabulary: {vocab_size} unique characters\")\n",
    "print(f\"  Total tokens: {len(all_tokens):,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Dataset: {DATASET_NAME}\")\n",
    "print(f\"üî§ Tokenization: {TOKENIZATION}\")\n",
    "print(f\"üìù Vocabulary: {vocab_size:,} tokens\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nüìñ Preview:\")\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1679888",
   "metadata": {},
   "source": [
    "## 3. Alternative Dataset Options (Commented)\n",
    "\n",
    "Uncomment any of these to try different datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# OPTION 2: WikiText-103 + GPT-2 Tokenizer\n",
    "# ----------------------------------------------------------------------------\n",
    "# Paper: \"Pointer Sentinel Mixture Models\"\n",
    "# Merity et al., Salesforce Research, 2016\n",
    "# https://arxiv.org/abs/1609.07843\n",
    "#\n",
    "# from datasets import load_dataset\n",
    "# from transformers import GPT2Tokenizer\n",
    "#\n",
    "# wikitext = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "#\n",
    "# all_tokens = []\n",
    "# for article in wikitext:\n",
    "#     tokens = tokenizer.encode(article['text'])\n",
    "#     all_tokens.extend(tokens)\n",
    "#\n",
    "# vocab_size = len(tokenizer)\n",
    "# DATASET_NAME = \"WikiText-103\"\n",
    "# TOKENIZATION = \"BPE (GPT-2)\"\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# OPTION 4: Custom Text File + SentencePiece\n",
    "# ----------------------------------------------------------------------------\n",
    "# Train custom tokenizer on your data\n",
    "# https://github.com/google/sentencepiece\n",
    "#\n",
    "# import sentencepiece as spm\n",
    "#\n",
    "# # Train SentencePiece model\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     input='your_data.txt',\n",
    "#     model_prefix='custom_tokenizer',\n",
    "#     vocab_size=4096,  # Adjustable\n",
    "#     character_coverage=0.9995,\n",
    "#     model_type='bpe'\n",
    "# )\n",
    "#\n",
    "# # Load trained tokenizer\n",
    "# tokenizer = spm.SentencePieceProcessor(model_file='custom_tokenizer.model')\n",
    "#\n",
    "# # Tokenize your text\n",
    "# with open('your_data.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "# all_tokens = tokenizer.encode(text, out_type=int)\n",
    "# vocab_size = tokenizer.vocab_size()\n",
    "#\n",
    "# DATASET_NAME = \"Custom Text\"\n",
    "# TOKENIZATION = \"SentencePiece BPE\"\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# OPTION 5: Character-level on Custom Text\n",
    "# ----------------------------------------------------------------------------\n",
    "# Simplest option - no dependencies\n",
    "#\n",
    "# with open('your_data.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "#\n",
    "# chars = sorted(list(set(text)))\n",
    "# char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "# idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "#\n",
    "# all_tokens = [char_to_idx[ch] for ch in text]\n",
    "# vocab_size = len(chars)\n",
    "#\n",
    "# DATASET_NAME = \"Custom Text\"\n",
    "# TOKENIZATION = \"Character-level\"\n",
    "\n",
    "print(\"Alternative dataset options available (see cell above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c1c6a2",
   "metadata": {},
   "source": [
    "## 4. Model Configuration and Creation\n",
    "\n",
    "Now we'll create the Hybrid Transformer model using the factory function from `hybrid_transformer1.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68215186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RSM MODEL CREATED\n",
      "================================================================================\n",
      "Vocabulary: 65 tokens\n",
      "Hidden size: 256\n",
      "Layers: 8\n",
      "Attention heads: 4\n",
      "Memory slots: 32\n",
      "Chunk size: 512\n",
      "Global sync: Yes\n",
      "\n",
      "Parameters:\n",
      "  Main model: 10,015,488\n",
      "  Global sync: 525,824\n",
      "  Total: 10,541,312\n",
      "  Memory (float32): 40.2 MB\n",
      "================================================================================\n",
      "\n",
      "‚úì Model ready for training\n",
      "‚úì Global sync layer: Enabled\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "hidden_size = 256          # Hidden dimension (d_model)\n",
    "num_heads = 4              # Number of attention heads\n",
    "num_layers = 8             # Number of transformer blocks (6, 8, 10)\n",
    "num_memory_slots = 32      # Number of external memory slots\n",
    "chunk_size = 512           # Context window size (max_seq_len)\n",
    "dropout = 0.1              # Dropout rate\n",
    "use_global_sync = True     # Whether to use global synchronization layer\n",
    "\n",
    "# Create model using factory function\n",
    "model, global_sync, config = create_rsm_model(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    num_memory_slots=num_memory_slots,\n",
    "    chunk_size=chunk_size,\n",
    "    dropout=dropout,\n",
    "    use_global_sync=use_global_sync,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Model is already created with summary printed by create_rsm_model()\n",
    "# Additional info:\n",
    "print(f\"\\n‚úì Model ready for training\")\n",
    "print(f\"‚úì Global sync layer: {'Enabled' if use_global_sync else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda3266",
   "metadata": {},
   "source": [
    "## 5. Prepare Training Data\n",
    "\n",
    "Create the dataset and dataloader using `ChunkedSequenceDataset` from `hybrid_transformer1.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1507ea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset prepared:\n",
      "  Total tokens: 1,115,394\n",
      "  Chunk size: 256\n",
      "  Batch size: 32\n",
      "  Number of batches: 273\n",
      "  Number of epochs: 10\n",
      "  Learning rate: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "chunk_size = 256       # Chunk size for training (context window per batch)[sequence_length]\n",
    "batch_size = 32        # Batch size\n",
    "num_epochs = 10        # Number of training epochs (50)\n",
    "learning_rate = 3e-4   # Learning rate\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = ChunkedSequenceDataset(\n",
    "    tokens=all_tokens,\n",
    "    chunk_size=chunk_size  # Fixed: was seq_length, should be chunk_size\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 for simpler debugging\n",
    ")\n",
    "\n",
    "print(f\"Training dataset prepared:\")\n",
    "print(f\"  Total tokens: {len(all_tokens):,}\")\n",
    "print(f\"  Chunk size: {chunk_size}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Number of batches: {len(train_loader):,}\")\n",
    "print(f\"  Number of epochs: {num_epochs}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f0ebe",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "Train the model using `train_rsm_epoch()` from `hybrid_transformer1.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a5950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # train_rsm_epoch returns a dict with 'loss', 'accuracy', 'num_chunks'\n",
    "    metrics = train_rsm_epoch(\n",
    "        model=model,\n",
    "        global_sync=global_sync,  # Required parameter\n",
    "        data_iterator=train_loader,  # Was train_loader, now data_iterator\n",
    "        optimizer=optimizer,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Extract metrics\n",
    "    loss = metrics['loss']\n",
    "    acc = metrics['accuracy'] * 100  # Convert to percentage\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(loss)\n",
    "    history['train_acc'].append(acc)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {loss:.4f} | Accuracy: {acc:.2f}%\")\n",
    "    \n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_checkpoint(\n",
    "            filepath=f'checkpoint_epoch_{epoch+1}.pt',\n",
    "            model=model,\n",
    "            global_sync=global_sync,\n",
    "            optimizer=optimizer,\n",
    "            config=config,\n",
    "            history=history\n",
    "        )\n",
    "        print(f\"  ‚Üí Checkpoint saved: checkpoint_epoch_{epoch+1}.pt\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Final Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {history['train_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf6a45",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Results\n",
    "\n",
    "Plot the loss and accuracy curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f057896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(history['train_acc'], label='Training Accuracy', color='green', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Training Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"  Best Loss: {min(history['train_loss']):.4f} (Epoch {history['train_loss'].index(min(history['train_loss']))+1})\")\n",
    "print(f\"  Best Accuracy: {max(history['train_acc']):.2f}% (Epoch {history['train_acc'].index(max(history['train_acc']))+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46341546",
   "metadata": {},
   "source": [
    "## 8. Text Generation\n",
    "\n",
    "Generate text samples using `generate_with_rsm()` from `hybrid_transformer1.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23739a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate text samples with different prompts (Shakespeare-themed)\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be\",\n",
    "    \"What light through yonder\"\n",
    "]\n",
    "\n",
    "print(\"Generated Text Samples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    # Encode prompt\n",
    "    if TOKENIZATION == \"Character-level\":\n",
    "        prompt_tokens = [char_to_idx.get(ch, 0) for ch in prompt]\n",
    "    else:\n",
    "        prompt_tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    # Generate\n",
    "    generated_tokens = generate_with_rsm(\n",
    "        model=model,\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    if TOKENIZATION == \"Character-level\":\n",
    "        generated_text = ''.join([idx_to_char.get(idx, '?') for idx in generated_tokens])\n",
    "    else:\n",
    "        generated_text = tokenizer.decode(generated_tokens)\n",
    "    \n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "    print(f\"Generated:\\n{generated_text}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nGeneration complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
