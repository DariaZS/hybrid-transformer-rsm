{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe03eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Import everything from the helper library\n",
    "from hybrid_transformer import *\n",
    "\n",
    "# Option 2: Import specific components\n",
    "# from hybrid_transformer import (\n",
    "#     HybridTransformer,\n",
    "#     NeedleInHaystackDataset,\n",
    "#     evaluate_random_baseline,\n",
    "#     count_parameters\n",
    "# )\n",
    "\n",
    "print(\"✓ Hybrid Transformer library loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5ea18b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch torchvision torchaudio\n",
    "#!pip install numpy matplotlib\n",
    "\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01692788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from torch.nn import LayerNorm\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b77618",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69090837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NeedleInHaystackDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A toy dataset for testing long-context memory.\n",
    "    \n",
    "    Task: Find a special key-value pair hidden in a long sequence.\n",
    "    Format: [random tokens...] <KEY> special_id <VALUE> target_value [random tokens...]\n",
    "    Goal: Given the sequence, predict the target_value associated with special_id\n",
    "    \n",
    "    This tests if the model can:\n",
    "    1. Identify the needle (key-value pair) in a long sequence\n",
    "    2. Store it in memory\n",
    "    3. Retrieve it later when queried\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples=1000, vocab_size=100, \n",
    "                 haystack_length=1000, num_needles=5, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples: Number of examples in dataset\n",
    "            vocab_size: Size of vocabulary (must be > num_needles + 3 for special tokens)\n",
    "            haystack_length: Total sequence length\n",
    "            num_needles: Number of key-value pairs to hide\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.vocab_size = vocab_size\n",
    "        self.haystack_length = haystack_length\n",
    "        self.num_needles = num_needles\n",
    "        \n",
    "        # Special tokens\n",
    "        self.PAD_TOKEN = 0\n",
    "        self.KEY_TOKEN = vocab_size - 3  # Special token indicating a key\n",
    "        self.VALUE_TOKEN = vocab_size - 2  # Special token indicating a value\n",
    "        self.QUERY_TOKEN = vocab_size - 1  # Special token for query\n",
    "        \n",
    "        # Key and value IDs (use tokens that won't be in haystack)\n",
    "        self.key_ids = list(range(vocab_size - 3 - num_needles * 2, \n",
    "                                   vocab_size - 3 - num_needles))\n",
    "        self.value_ids = list(range(vocab_size - 3 - num_needles, \n",
    "                                     vocab_size - 3))\n",
    "        \n",
    "        # Tokens available for haystack (excluding special tokens and key/value IDs)\n",
    "        self.haystack_vocab = list(range(1, vocab_size - 3 - num_needles * 2))\n",
    "        \n",
    "        random.seed(seed)\n",
    "        self.data = self._generate_dataset()\n",
    "    \n",
    "    def _generate_dataset(self):\n",
    "        \"\"\"Generate all samples\"\"\"\n",
    "        data = []\n",
    "        for _ in range(self.num_samples):\n",
    "            sample = self._generate_sample()\n",
    "            data.append(sample)\n",
    "        return data\n",
    "    \n",
    "    def _generate_sample(self):\n",
    "        \"\"\"\n",
    "        Generate a single sample with format:\n",
    "        [haystack] <KEY> key_id <VALUE> value_id [haystack] ... <QUERY> key_id -> target: value_id\n",
    "        \"\"\"\n",
    "        # Create needle pairs (key-value associations)\n",
    "        needle_pairs = list(zip(self.key_ids[:self.num_needles], \n",
    "                                self.value_ids[:self.num_needles]))\n",
    "        random.shuffle(needle_pairs)\n",
    "        \n",
    "        # Choose which needle to query\n",
    "        query_key, query_value = random.choice(needle_pairs)\n",
    "        \n",
    "        # Calculate space for haystack\n",
    "        # Each needle takes 4 tokens: <KEY> key_id <VALUE> value_id\n",
    "        # Query takes 2 tokens: <QUERY> key_id\n",
    "        needle_tokens = self.num_needles * 4\n",
    "        query_tokens = 2\n",
    "        available_haystack = self.haystack_length - needle_tokens - query_tokens\n",
    "        \n",
    "        # Distribute haystack tokens\n",
    "        haystack_segments = []\n",
    "        remaining = available_haystack\n",
    "        for i in range(self.num_needles + 1):  # One segment before each needle, one after\n",
    "            segment_length = remaining // (self.num_needles + 1 - i)\n",
    "            haystack_segments.append(\n",
    "                [random.choice(self.haystack_vocab) for _ in range(segment_length)]\n",
    "            )\n",
    "            remaining -= segment_length\n",
    "        \n",
    "        # Build sequence: interleave haystack and needles\n",
    "        sequence = []\n",
    "        for i, (key_id, value_id) in enumerate(needle_pairs):\n",
    "            sequence.extend(haystack_segments[i])\n",
    "            sequence.extend([self.KEY_TOKEN, key_id, self.VALUE_TOKEN, value_id])\n",
    "        sequence.extend(haystack_segments[-1])  # Final haystack segment\n",
    "        \n",
    "        # Add query at the end\n",
    "        sequence.extend([self.QUERY_TOKEN, query_key])\n",
    "        \n",
    "        # Target is the value associated with the queried key\n",
    "        target = query_value\n",
    "        \n",
    "        return {\n",
    "            'input': torch.tensor(sequence, dtype=torch.long),\n",
    "            'target': torch.tensor(target, dtype=torch.long),\n",
    "            'needle_pairs': needle_pairs,\n",
    "            'query_key': query_key\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function for DataLoader\"\"\"\n",
    "        inputs = torch.stack([item['input'] for item in batch])\n",
    "        targets = torch.stack([item['target'] for item in batch])\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f4586fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random baseline accuracy: 20.00%\n",
      "Expected random accuracy: 33.33% (1/3)\n",
      "\n",
      "This is what we need to beat! The model must learn to:\n",
      "  1. Identify and memorize the key-value pairs\n",
      "  2. Recognize the query key\n",
      "  3. Retrieve the correct value from memory\n"
     ]
    }
   ],
   "source": [
    "# Random baseline\n",
    "def evaluate_random_baseline(dataset):\n",
    "    \"\"\"Evaluate random guessing performance\"\"\"\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    \n",
    "    for i in range(total):\n",
    "        sample = dataset[i]\n",
    "        # Random guess from possible values\n",
    "        random_guess = random.choice(dataset.value_ids)\n",
    "        if random_guess == sample['target'].item():\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    return accuracy\n",
    "\n",
    "random_acc = evaluate_random_baseline(test_dataset)\n",
    "print(f\"Random baseline accuracy: {random_acc:.2f}%\")\n",
    "print(f\"Expected random accuracy: {100/test_dataset.num_needles:.2f}% (1/{test_dataset.num_needles})\")\n",
    "print(f\"\\nThis is what we need to beat! The model must learn to:\")\n",
    "print(\"  1. Identify and memorize the key-value pairs\")\n",
    "print(\"  2. Recognize the query key\")\n",
    "print(\"  3. Retrieve the correct value from memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aaced4",
   "metadata": {},
   "source": [
    "### Baseline: Random Guessing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6b230dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 125\n",
      "Validation batches: 25\n",
      "\n",
      "Batch shapes:\n",
      "  Inputs: torch.Size([8, 512])\n",
      "  Targets: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation datasets\n",
    "train_dataset = NeedleInHaystackDataset(\n",
    "    num_samples=1000,\n",
    "    vocab_size=100,\n",
    "    haystack_length=512,  # Medium length for initial training\n",
    "    num_needles=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_dataset = NeedleInHaystackDataset(\n",
    "    num_samples=200,\n",
    "    vocab_size=100,\n",
    "    haystack_length=512,\n",
    "    num_needles=5,\n",
    "    seed=123  # Different seed for validation\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_dataset.collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=val_dataset.collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Test loading a batch\n",
    "inputs, targets = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  Inputs: {inputs.shape}\")\n",
    "print(f\"  Targets: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b87a4ef",
   "metadata": {},
   "source": [
    "### Create DataLoader for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "536149a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "Total samples: 5\n",
      "Vocab size: 50\n",
      "Haystack length: 100\n",
      "Number of needles: 3\n",
      "Special tokens: KEY=47, VALUE=48, QUERY=49\n",
      "\n",
      "Key IDs: [41, 42, 43]\n",
      "Value IDs: [44, 45, 46]\n",
      "\n",
      "============================================================\n",
      "Sample Sequence Breakdown:\n",
      "============================================================\n",
      "Total sequence length: 100\n",
      "Target value: 45\n",
      "Needle pairs in this sample: [(42, 45), (41, 44), (43, 46)]\n",
      "Queried key: 42\n",
      "\n",
      "Needles found in sequence:\n",
      "  Position  21: <KEY> 42 <VALUE> 45 ← QUERIED\n",
      "  Position  46: <KEY> 41 <VALUE> 44 \n",
      "  Position  72: <KEY> 43 <VALUE> 46 \n",
      "\n",
      "Query at position 98: <QUERY> 42 -> Expected answer: 45\n",
      "\n",
      "============================================================\n",
      "Sequence snippet (first 50 tokens):\n",
      "============================================================\n",
      "[18, 16, 15, 9, 7, 35, 6, 38, 28, 3, 2, 6, 14, 15, 33, 39, 2, 36, 13, 35, 27, 47, 42, 48, 45, 15, 29, 38, 18, 1, 11, 28, 22, 18, 10, 14, 22, 7, 6, 25, 7, 23, 23, 39, 17, 3, 47, 41, 48, 44]\n"
     ]
    }
   ],
   "source": [
    "# Create a small dataset to visualize\n",
    "test_dataset = NeedleInHaystackDataset(\n",
    "    num_samples=5,\n",
    "    vocab_size=50,\n",
    "    haystack_length=100,\n",
    "    num_needles=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Total samples: {len(test_dataset)}\")\n",
    "print(f\"Vocab size: {test_dataset.vocab_size}\")\n",
    "print(f\"Haystack length: {test_dataset.haystack_length}\")\n",
    "print(f\"Number of needles: {test_dataset.num_needles}\")\n",
    "print(f\"Special tokens: KEY={test_dataset.KEY_TOKEN}, VALUE={test_dataset.VALUE_TOKEN}, QUERY={test_dataset.QUERY_TOKEN}\")\n",
    "print(f\"\\nKey IDs: {test_dataset.key_ids}\")\n",
    "print(f\"Value IDs: {test_dataset.value_ids}\")\n",
    "\n",
    "# Examine one sample\n",
    "sample = test_dataset[0]\n",
    "sequence = sample['input']\n",
    "target = sample['target']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Sample Sequence Breakdown:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total sequence length: {len(sequence)}\")\n",
    "print(f\"Target value: {target.item()}\")\n",
    "print(f\"Needle pairs in this sample: {sample['needle_pairs']}\")\n",
    "print(f\"Queried key: {sample['query_key']}\")\n",
    "\n",
    "# Find and display the needles in the sequence\n",
    "print(f\"\\nNeedles found in sequence:\")\n",
    "seq_list = sequence.tolist()\n",
    "for i, token in enumerate(seq_list):\n",
    "    if token == test_dataset.KEY_TOKEN:\n",
    "        key_id = seq_list[i+1]\n",
    "        value_id = seq_list[i+3]\n",
    "        is_queried = \"← QUERIED\" if key_id == sample['query_key'] else \"\"\n",
    "        print(f\"  Position {i:3d}: <KEY> {key_id} <VALUE> {value_id} {is_queried}\")\n",
    "\n",
    "# Display query\n",
    "query_pos = seq_list.index(test_dataset.QUERY_TOKEN)\n",
    "query_key = seq_list[query_pos + 1]\n",
    "print(f\"\\nQuery at position {query_pos}: <QUERY> {query_key} -> Expected answer: {target.item()}\")\n",
    "\n",
    "# Show a snippet of the sequence\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Sequence snippet (first 50 tokens):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(seq_list[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c673c1f",
   "metadata": {},
   "source": [
    "### Test the Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b5e06a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 50])\n",
      "Output logits shape: torch.Size([2, 50, 100])\n",
      "Memory state shape: torch.Size([2, 8, 64])\n",
      "Model has 97920 parameters\n",
      "\n",
      "✓ Model instantiated and forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "vocab_size = 100\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "n_slots = 8\n",
    "window_size = 128\n",
    "\n",
    "model = HybridTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    n_slots=n_slots,\n",
    "    window_size=window_size\n",
    ")\n",
    "\n",
    "# Create dummy input\n",
    "batch_size = 2\n",
    "seq_len = 50\n",
    "inputs = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Forward pass\n",
    "logits, memory = model(inputs)\n",
    "\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Memory state shape: {memory.shape}\")\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(\"\\n✓ Model instantiated and forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0726e",
   "metadata": {},
   "source": [
    "### Test Model Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a398f79",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing & Evaluation\n",
    "\n",
    "Now that all components are defined, let's test the model and dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f49c2f",
   "metadata": {},
   "source": [
    "## Needle-in-Haystack Dataset\n",
    "\n",
    "This toy task tests whether the model can find and retrieve a specific \"needle\" (key-value pair) hidden in a long sequence of \"haystack\" (random tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e91fbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, n_slots=16, window_size=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_emb = PositionalEncoding(hidden_size)\n",
    "        \n",
    "        # Initialize memory slots as learnable embeddings\n",
    "        self.memory_init = nn.Parameter(torch.randn(1, n_slots, hidden_size) * 0.02)\n",
    "        \n",
    "        # Stack of hybrid blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            HybridTransformerBlock(hidden_size, n_slots, window_size) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        self.head.weight = self.tok_emb.weight  # Weight tying\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_slots = n_slots\n",
    "        \n",
    "    def forward(self, inputs, memory_state=None):\n",
    "        \"\"\"\n",
    "        inputs: shape (batch, seq_len)\n",
    "        memory_state: optional, shape (batch, n_slots, hidden_size)\n",
    "        Returns: logits (batch, seq_len, vocab_size), final_memory\n",
    "        \"\"\"\n",
    "        batch_size = inputs.shape[0]\n",
    "        seq_len = inputs.shape[1]\n",
    "        \n",
    "        # Initialize memory if not provided\n",
    "        if memory_state is None:\n",
    "            memory_state = self.memory_init.expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Embeddings\n",
    "        pos = torch.arange(0, seq_len, device=inputs.device).long()\n",
    "        x = self.tok_emb(inputs) + self.pos_emb(pos)\n",
    "        \n",
    "        # Pass through transformer blocks with memory\n",
    "        for block in self.blocks:\n",
    "            x, memory_state = block(x, memory_state)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits, memory_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5bba89",
   "metadata": {},
   "source": [
    "## Full Hybrid Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "203d6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridTransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, n_slots=16, window_size=512):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm(hidden_size)\n",
    "        self.norm2 = LayerNorm(hidden_size)\n",
    "        self.norm3 = LayerNorm(hidden_size)\n",
    "        \n",
    "        # Components you already know\n",
    "        self.local_attention = WindowedAttention(hidden_size, window_size)\n",
    "        \n",
    "        # New components for memory\n",
    "        self.memory_read = CrossAttention(hidden_size, n_slots)\n",
    "        self.memory_write = GatedSSM(hidden_size, n_slots)\n",
    "        \n",
    "        # MLP from your assignment\n",
    "        self.mlp = MLP(hidden_size)\n",
    "        \n",
    "    def forward(self, x, memory_state):\n",
    "        \"\"\"\n",
    "        x: shape (batch, seq_len, hidden_size)\n",
    "        memory_state: shape (batch, n_slots, hidden_size)\n",
    "        Returns: output (batch, seq_len, hidden_size), new_memory (batch, n_slots, hidden_size)\n",
    "        \"\"\"\n",
    "        # 1. Local windowed attention (you've done this before)\n",
    "        h_local, _ = self.local_attention(self.norm1(x))\n",
    "        h = x + h_local\n",
    "        \n",
    "        # 2. Read from memory using cross-attention\n",
    "        context = self.memory_read(self.norm2(h), memory_state)\n",
    "        h = h + context  # Add memory context to hidden states\n",
    "        \n",
    "        # 3. MLP (standard transformer component)\n",
    "        h = h + self.mlp(self.norm3(h))\n",
    "        \n",
    "        # 4. Write to memory (update memory state)\n",
    "        new_memory = self.memory_write(h, memory_state)\n",
    "        \n",
    "        return h, new_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d9060",
   "metadata": {},
   "source": [
    "## Hybrid Transformer Block\n",
    "\n",
    "This combines local attention with memory read/write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8811e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedSSM(nn.Module):\n",
    "    \"\"\"Gated State Space Model for writing to memory\"\"\"\n",
    "    def __init__(self, hidden_size, n_slots):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_slots = n_slots\n",
    "        \n",
    "        # Gate parameters: decide how much to update memory\n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Update parameters: what to write to memory\n",
    "        self.A = nn.Linear(hidden_size, hidden_size, bias=False)  # Memory recurrence\n",
    "        self.B = nn.Linear(hidden_size, hidden_size, bias=False)  # Input projection\n",
    "        \n",
    "    def forward(self, h, memory_state):\n",
    "        \"\"\"\n",
    "        h: shape (batch, seq_len, hidden_size) - current hidden states\n",
    "        memory_state: shape (batch, n_slots, hidden_size) - current memory\n",
    "        Returns: new_memory (batch, n_slots, hidden_size)\n",
    "        \n",
    "        Implements: M_{t+1} = (1 - G_t) ⊙ M_t + G_t ⊙ φ(AM_t + BU_t)\n",
    "        \"\"\"\n",
    "        # TODO: Implement gated memory update\n",
    "        \n",
    "        # Summarize the sequence for memory update (simple mean pooling)\n",
    "        summary = h.mean(dim=1, keepdim=True)  # (batch, 1, hidden_size)\n",
    "        summary = summary.expand(-1, self.n_slots, -1)  # (batch, n_slots, hidden_size)\n",
    "        \n",
    "        # Compute gate: how much to update each memory slot\n",
    "        gate = self.gate_net(summary)  # (batch, n_slots, hidden_size)\n",
    "        \n",
    "        # Compute update: what to write to memory\n",
    "        update = torch.tanh(self.A(memory_state) + self.B(summary))\n",
    "        \n",
    "        # Blend old and new memory\n",
    "        new_memory = (1 - gate) * memory_state + gate * update\n",
    "        \n",
    "        return new_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ea27059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention for reading from memory slots\"\"\"\n",
    "    def __init__(self, hidden_size, n_slots):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_slots = n_slots\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        \"\"\"\n",
    "        x: shape (batch, seq_len, hidden_size) - queries from input\n",
    "        memory: shape (batch, n_slots, hidden_size) - keys/values from memory\n",
    "        Returns: context (batch, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # TODO: Implement cross-attention to read from memory\n",
    "        # This is similar to your Attention class but queries come from x\n",
    "        # and keys/values come from memory\n",
    "        \n",
    "        q = self.Q(x)  # (batch, seq_len, hidden_size)\n",
    "        k = self.K(memory)  # (batch, n_slots, hidden_size)\n",
    "        v = self.V(memory)  # (batch, n_slots, hidden_size)\n",
    "        \n",
    "        context, attention = scaled_dot_attention(q, k, v, mask=0)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff18c1",
   "metadata": {},
   "source": [
    "## Memory Components (New - To Implement)\n",
    "\n",
    "These handle reading from and writing to external memory slots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8e7d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedAttention(nn.Module):\n",
    "    \"\"\"Local attention within a sliding window\"\"\"\n",
    "    def __init__(self, hidden_size, window_size=512):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (batch, seq_len, hidden_size)\n",
    "        Returns: context (batch, seq_len, hidden_size), attention weights\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        \n",
    "        # TODO: Implement windowed attention\n",
    "        # For now, just doing causal attention as placeholder\n",
    "        # You'll need to create a mask that only attends to nearby tokens\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        \n",
    "        # Simple approach: full causal attention (to be replaced with windowed)\n",
    "        unnorm_attn = torch.matmul(q, k.transpose(-2, -1))\n",
    "        unnorm_attn = unnorm_attn / math.sqrt(self.hidden_size)\n",
    "        masked_unnorm_attn = unnorm_attn + mask * -1e9\n",
    "        attention = torch.softmax(masked_unnorm_attn, dim=-1)\n",
    "        context = torch.matmul(attention, v)\n",
    "        \n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53171e65",
   "metadata": {},
   "source": [
    "## Windowed Attention (New Component)\n",
    "\n",
    "This implements local attention within a sliding window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f897e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size, max_len = 1000):\n",
    "        super().__init__()\n",
    "        pos = torch.arange(max_len).float().unsqueeze(1)\n",
    "        dim = torch.arange(hidden_size // 2).float().unsqueeze(0)\n",
    "        div_term = torch.exp(-math.log(10000.0) * (2 * dim) / hidden_size)\n",
    "        angle = pos * div_term\n",
    "        pe = torch.zeros(max_len, hidden_size)\n",
    "        pe[:, 0::2] = torch.sin(angle)\n",
    "        pe[:, 1::2] = torch.cos(angle)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        return self.pe[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1  = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.layer2  = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.relu    = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5012872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_attention(q, k, v, mask=0):\n",
    "    \"\"\"Computes scaled dot product attention with an optional mask.\n",
    "    q : shape ( batch, k, hidden_size)\n",
    "    k : shape ( batch, seq_len, hidden_size)\n",
    "    v : shape ( batch, seq_len, hidden_size)\n",
    "    mask: optional. shape (k, seq_len)\n",
    "\n",
    "    returns:\n",
    "        context   : shape (batch, k, hidden_size)\n",
    "        attention : shape (batch, k, seq_len)\n",
    "    \"\"\"\n",
    "    unnorm_attn = torch.matmul(q, k.transpose(-2, -1))\n",
    "    unnorm_attn = unnorm_attn/torch.sqrt(torch.tensor(q.shape[-1], dtype=torch.float32))\n",
    "    masked_unnorm_attn = unnorm_attn + mask*-1e9\n",
    "    attention = torch.softmax(masked_unnorm_attn, dim = -1)\n",
    "    context = torch.matmul(attention, v)\n",
    "    return context, attention\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Standard attention mechanism\"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, annots):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(annots)\n",
    "        v = self.V(annots)\n",
    "        return scaled_dot_attention(q, k, v)\n",
    "\n",
    "\n",
    "class CausalAttention(Attention):\n",
    "    \"\"\"Causal (autoregressive) attention\"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        seq_len = x.shape[1]\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal = 1)\n",
    "        return scaled_dot_attention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02003b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58149a37",
   "metadata": {},
   "source": [
    "# Hybrid Transformer with Memory Slots\n",
    "\n",
    "This notebook implements a transformer with external memory slots for long-context processing, building on components from the NN assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
